{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ca3603-431d-48a8-9cb4-c1efcdd190d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from pinecone-client) (2025.10.5)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from pinecone-client) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from pinecone-client) (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# [Cell 1] Install necessary libraries\n",
    "# We need sentence-transformers for embeddings and pinecone as our vector DB.\n",
    "!pip install sentence-transformers pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd758165-3836-45c9-9250-c730faf2a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 2] Import libraries\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "from getpass import getpass # To securely ask for your API key\n",
    "from tqdm.auto import tqdm # Shows a progress bar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18f1989-54da-44fb-9f93-844c5f453435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Total products: 312\n"
     ]
    }
   ],
   "source": [
    "# [Cell 3] Load the dataset\n",
    "# We reload the dataset to start fresh in this notebook.\n",
    "df = pd.read_csv('products_dataset.csv')\n",
    "\n",
    "# CRITICAL: We must handle missing values in the text fields, or the model will fail.\n",
    "# We'll fill 'NaN' (missing values) with an empty string.\n",
    "df = df.fillna(\"\")\n",
    "print(f\"Dataset loaded. Total products: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b8d5b90-1a43-4d5f-b5fe-a958d10f9693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Embedding dimension:  384\n"
     ]
    }
   ],
   "source": [
    "# [Cell 4] Load the NLP Model (all-MiniLM-L6-v2)\n",
    "# This model is excellent at creating 'embeddings' (vector representations) for text.\n",
    "# It maps sentences with similar meanings to similar vectors.\n",
    "# This model creates 384-dimensional vectors.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded. Embedding dimension: \", model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe6a4ea-7767-47cf-9707-c1fbcca164d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Pinecone API Key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'product-recommender' already exists.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 5] Initialize Pinecone (NEW SYNTAX)\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from getpass import getpass\n",
    "\n",
    "# It will ask for your API key. Paste it in and press Enter.\n",
    "PINECONE_API_KEY = getpass(\"Enter your Pinecone API Key: \")\n",
    "\n",
    "# These are from your screenshot\n",
    "PINECONE_REGION = \"us-east-1\"\n",
    "PINECONE_CLOUD = \"aws\"\n",
    "\n",
    "# 1. Initialize the Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = 'product-recommender'\n",
    "\n",
    "# 2. Check if the index already exists.\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating new index: {index_name}\")\n",
    "    # 3. Create the index with the new 'spec' format\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=model.get_sentence_embedding_dimension(), # This will be 384\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=PINECONE_CLOUD,\n",
    "            region=PINECONE_REGION\n",
    "        )\n",
    "    )\n",
    "    print(f\"Index '{index_name}' created.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# 4. Connect to the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4b07d5-bcde-454e-ac11-4f012bab48e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to generate embeddings and upload to Pinecone...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10a9ca346b448f5a52b5eb16d54873b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All products have been embedded and uploaded to Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 6] Generate Embeddings and Upload to Pinecone\n",
    "# This is the main loop. We'll go through the CSV in batches.\n",
    "from tqdm.auto import tqdm # Shows a progress bar\n",
    "\n",
    "batch_size = 100 # We'll process 100 products at a time\n",
    "\n",
    "print(\"Starting to generate embeddings and upload to Pinecone...\")\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    # Get a batch of rows from the dataframe\n",
    "    i_end = min(i + batch_size, len(df))\n",
    "    batch = df.iloc[i:i_end]\n",
    "    \n",
    "    # 1. Create a combined text field for embedding\n",
    "    # This IS the NLP step: combining features to create a rich description.\n",
    "    combined_text = (\n",
    "        \"Title: \" + batch['title'] +\n",
    "        \"; Brand: \" + batch['brand'] +\n",
    "        \"; Categories: \" + batch['categories'] +\n",
    "        \"; Description: \" + batch['description']\n",
    "    ).tolist()\n",
    "    \n",
    "    # 2. Generate embeddings (ML Step)\n",
    "    embeddings = model.encode(combined_text).tolist()\n",
    "    \n",
    "    # 3. Prepare data for Pinecone\n",
    "    # We must include the unique ID, the embedding 'values', and 'metadata'\n",
    "    # The metadata is what we'll get back from our search.\n",
    "    to_upsert = []\n",
    "    for idx, row in batch.iterrows():\n",
    "        to_upsert.append({\n",
    "            \"id\": row['uniq_id'],\n",
    "            \"values\": embeddings[idx - i], # Get the embedding for this row\n",
    "            \"metadata\": {\n",
    "                \"title\": row['title'],\n",
    "                \"price\": row['price'],\n",
    "                # Get the first image URL, or empty string if none\n",
    "                \"image_url\": (row['images'].split(',')[0] if row['images'] else \"\"),\n",
    "                \"brand\": row['brand'],\n",
    "                \"categories\": row['categories']\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # 4. Upload the batch to Pinecone\n",
    "    index.upsert(vectors=to_upsert)\n",
    "\n",
    "print(\"All products have been embedded and uploaded to Pinecone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "689693d4-2628-45aa-9449-c421d3edcb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (12.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from torchvision) (2.3.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taman\\product-recommendation-app\\backend-env\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# [Cell 7] Install PyTorch and other libraries for CV\n",
    "!pip install torch torchvision Pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "200e2381-1efa-46bb-bd14-2ac05d13d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# [Cell 8] Import CV libraries\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Let's check if a GPU is available, otherwise we'll use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb4e0bba-0dcb-4962-8e7f-67dd5669b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Top 5 Cleaned Categories:\n",
      "clean_category\n",
      "Furniture                 203\n",
      "Outdoor Décor              22\n",
      "Storage & Organization     21\n",
      "Home Décor Products        21\n",
      "Hardware                   17\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# [Cell 9 - New Cell] Clean the categories column\n",
    "import ast # This library safely evaluates string-lists\n",
    "\n",
    "def clean_category(cat_str):\n",
    "    try:\n",
    "        # Safely evaluate the string as a Python list\n",
    "        cat_list = ast.literal_eval(cat_str)\n",
    "        \n",
    "        # Heuristic: Take the *second* element as the main category\n",
    "        # (e.g., ['Home & Kitchen', 'Furniture', ...]) -> 'Furniture'\n",
    "        # If it's shorter, take the last element.\n",
    "        if isinstance(cat_list, list) and len(cat_list) > 1:\n",
    "            return cat_list[1] # e.g., 'Furniture'\n",
    "        elif isinstance(cat_list, list) and len(cat_list) > 0:\n",
    "            return cat_list[-1] # e.g., 'Doormats'\n",
    "        else:\n",
    "            return \"Uncategorized\" # Fallback\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If it's not a list-string, just return the original string or a fallback\n",
    "        return cat_str if pd.notna(cat_str) else \"Uncategorized\"\n",
    "\n",
    "# Apply this function to create a new column\n",
    "df['clean_category'] = df['categories'].apply(clean_category)\n",
    "\n",
    "# Let's check our new, cleaner top categories!\n",
    "print(\"New Top 5 Cleaned Categories:\")\n",
    "print(df['clean_category'].value_counts().nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9411adbe-daf4-4cdd-b6a7-a306eb29f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 clean categories: ['Furniture', 'Outdoor Décor', 'Storage & Organization', 'Home Décor Products', 'Hardware']\n",
      "Downloading 97 sample images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6075bc0b420548c1a1b8ca28f02ab301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image download complete.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 10 - CORRECTED] Define a function to download our sample images\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "import ast # Import ast to clean the string-lists\n",
    "\n",
    "DATA_PATH = \"data/cv_training\"\n",
    "\n",
    "def download_sample_images(df, num_per_category=20):\n",
    "    # Use the 'clean_category' column\n",
    "    top_5_categories = df['clean_category'].value_counts().nlargest(5).index.tolist()\n",
    "    print(f\"Top 5 clean categories: {top_5_categories}\")\n",
    "    \n",
    "    sample_df_list = []\n",
    "    for category in top_5_categories:\n",
    "        category_df = df[df['clean_category'] == category]\n",
    "        \n",
    "        # Safety check\n",
    "        category_count = len(category_df)\n",
    "        n_samples = min(num_per_category, category_count)\n",
    "        \n",
    "        if n_samples > 0:\n",
    "            sample_df_list.append(category_df.sample(n=n_samples, random_state=42))\n",
    "    \n",
    "    if not sample_df_list:\n",
    "        print(\"No samples to download. Check your category cleaning.\")\n",
    "        return []\n",
    "\n",
    "    sample_df = pd.concat(sample_df_list)\n",
    "    print(f\"Downloading {len(sample_df)} sample images...\")\n",
    "    \n",
    "    # Create directories and download\n",
    "    for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        category_name = str(row['clean_category']).replace('/', '_').replace('&', 'and')\n",
    "        category_path = os.path.join(DATA_PATH, category_name)\n",
    "        \n",
    "        if not os.path.exists(category_path):\n",
    "            os.makedirs(category_path)\n",
    "        \n",
    "        # === START OF NEW FIX ===\n",
    "        img_url = \"\"\n",
    "        try:\n",
    "            # 1. Safely evaluate the string as a Python list\n",
    "            img_list = ast.literal_eval(row['images'])\n",
    "            \n",
    "            # 2. Check if the list is not empty and get the first URL\n",
    "            if isinstance(img_list, list) and len(img_list) > 0:\n",
    "                img_url = img_list[0].strip() # 3. .strip() removes extra spaces\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Fallback if it's not a list (e.g., just a single URL string)\n",
    "            img_url = row['images'].split(',')[0].strip()\n",
    "        # === END OF NEW FIX ===\n",
    "\n",
    "        if not img_url:\n",
    "            print(f\"Skipping product {row['uniq_id']} - No valid image URL found.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(img_url, timeout=5)\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                img.save(os.path.join(category_path, f\"{row['uniq_id']}.jpg\"))\n",
    "            else:\n",
    "                print(f\"Could not download {img_url} - Status Code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not download {img_url}: {e}\")\n",
    "            \n",
    "    print(\"Image download complete.\")\n",
    "    return top_5_categories\n",
    "\n",
    "# Run the new and improved function\n",
    "top_5_categories = download_sample_images(df, num_per_category=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69d678db-f0ab-49c2-9cd9-73a37b1cf6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. Using 5 classes: ['Furniture', 'Hardware', 'Home Décor Products', 'Outdoor Décor', 'Storage and Organization']\n",
      "Training samples: 75, Validation samples: 19\n"
     ]
    }
   ],
   "source": [
    "# [Cell 11 - CORRECTED] Create a PyTorch Dataset and DataLoaders\n",
    "# We'll use a standard ImageFolder dataset, which is perfect for this structure.\n",
    "# We also define transforms to make all images the same size and normalize them.\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# --- FIX: Import 'datasets' from torchvision ---\n",
    "from torchvision import transforms, models, datasets\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- FIX: Use 'datasets.ImageFolder' instead of 'models.folder.ImageFolder' ---\n",
    "# Load the dataset from the folder we just created\n",
    "full_dataset = datasets.ImageFolder(root=DATA_PATH, transform=transform)\n",
    "\n",
    "# Split into training and validation (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Save the class names (so we know what category '0', '1', '2' etc. mean)\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Dataset created. Using {len(class_names)} classes: {class_names}\")\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c940818-4045-4b3d-98a0-fdca4a011478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\taman/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:12<00:00, 3.66MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-18 model loaded and final layer replaced.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# [Cell 12] Load the pre-trained CV Model (ResNet-18)\n",
    "# We use 'transfer learning' - using a model already trained on millions of images\n",
    "# and just fine-tuning the last layer for our 5 categories.\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "model_cv = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the final one\n",
    "for param in model_cv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer to output 5 classes (our top 5 categories)\n",
    "num_ftrs = model_cv.fc.in_features\n",
    "model_cv.fc = nn.Linear(num_ftrs, len(class_names)) # len(class_names) is 5\n",
    "\n",
    "model_cv = model_cv.to(device) # Move the model to the GPU if available\n",
    "print(\"ResNet-18 model loaded and final layer replaced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89693848-f74a-47b5-8619-2a20ceddb5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CV model training...\n",
      "Epoch 1/10, Loss: 1.8886\n",
      "Epoch 2/10, Loss: 1.6903\n",
      "Epoch 3/10, Loss: 1.4433\n",
      "Epoch 4/10, Loss: 1.2180\n",
      "Epoch 5/10, Loss: 1.0967\n",
      "Epoch 6/10, Loss: 0.9351\n",
      "Epoch 7/10, Loss: 0.8242\n",
      "Epoch 8/10, Loss: 0.7380\n",
      "Epoch 9/10, Loss: 0.6489\n",
      "Epoch 10/10, Loss: 0.6073\n",
      "Training complete.\n",
      "CV Model saved as 'cv_classifier_model.pth' in your project folder.\n"
     ]
    }
   ],
   "source": [
    "# [Cell 13] Train the CV Model\n",
    "# This cell will run for a few minutes. It will print the progress for each 'epoch' (pass).\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# We only want to optimize the parameters of the new final layer\n",
    "optimizer = optim.Adam(model_cv.fc.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "print(\"Starting CV model training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_cv.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. Clear old gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass: Get model predictions\n",
    "        outputs = model_cv(inputs)\n",
    "        \n",
    "        # 3. Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 4. Backward pass: Calculate gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# [Cell 14 - Save the model]\n",
    "# This saves our 'perfect' CV model.\n",
    "torch.save(model_cv.state_dict(), 'cv_classifier_model.pth')\n",
    "print(f\"CV Model saved as 'cv_classifier_model.pth' in your project folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d617e-a2e5-4f3d-ba88-ff8a36ec24e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
